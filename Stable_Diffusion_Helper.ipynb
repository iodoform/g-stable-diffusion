{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/iodoform/g-stable-diffusion/blob/main/Stable_Diffusion_Helper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"i3Oeq07oJ28B"},"source":["# AI無限ガチャ\n","\n","画像生成AI [StableDiffusion](https://github.com/CompVis/stable-diffusion)を使って無限に回せるガチャを作りました。\n","\n","AI画像生成部分は[@fladdict](https://twitter.com/fladdict)氏のカスタム版[stable-diffusion](https://github.com/fladdict/stable-diffusion)からフォークさせていただきました。\n","\n","下記の**重要**、**利用前の注意**、**お願い**、**謝辞**は@fladdict氏が書いたものをそのまま残してあります。\n","\n","ご利用の際は順守をお願いします。"]},{"cell_type":"markdown","metadata":{"id":"dBx4NIUO-B-2"},"source":["## 使い方\n","\n","* このページ上部のメニューで、「ランタイム > ランタイムのタイプを変更」からGPUを有効化\n","* [HuggingFace](https://huggingface.co/)でアカウントを作成\n","* [StableDiffusionのモデルページ](https://huggingface.co/CompVis/stable-diffusion-v-1-4-original)で、「利用規約」に合意する。\n","* モデルファイル [sd-v1-4.ckpt](https://huggingface.co/CompVis/stable-diffusion-v-1-4-original/resolve/main/sd-v1-4.ckpt) をダウンロード\n","* モデルファイルを Google Drive等にアップロード\n","* 下のセル 「1-1. Google Driveとの接続」を実行\n","* 下のセル　「1-2. のフォーム」に、Google Driveにアップしたモデルのパスをセット\n","* このページ上部のメニューで、「ランタイム > 全てのセルを実行」を選択\n","\n","## 不安程な場合\n","* CUDA Errorが出る場合、メモリが足りてないのでGoogle Colab Pro（or Pro+)を検討ください。"]},{"cell_type":"markdown","metadata":{"id":"Kq1JsCJiuQ58"},"source":["## 重要\n","\n","「[Stable Diffusionの利用ライセンス](https://huggingface.co/spaces/CompVis/stable-diffusion-license)」を遵守してご利用ください。\n","\n","----\n","\n","## 利用前の注意\n","画像生成AIは、インターネットそのものの縮図です。あらゆるものを生成するので、生成者は自分の生成物に責任をもつ必要があります。\n","\n","多くの場合、問題ある画像は偶発的というよりは、「生成者が意図的に指示」をすることで生成されます。以下のようなことを心がけましょう。\n","\n","\n","* ポルノを含む、性的な画像を生成しない（海外基準で罰せられる可能性があります）。\n","* 攻撃的な画像、差別的な画像、人を不快にする目的の画像を生成しない。\n","* 政治的な主張に用いない。\n","* 各種の文化バイアスがかかる場合があります。生成者が適宜バランスを調整をする（例、「結婚式」の画像は欧米式で異性愛の画像になりやすい。医者の画像は白人男性になりやすい、他人の著作物をアップロードしない）。\n","* 他者の権利を侵害しない（孫悟空やダースベイダーなどを意図的に作らない）。\n","* 実材の人物、事件、イベントの画像（フェイクニュース含む）を作成しない。\n","* 現役の作家の画風を単独指名で入力しない（個人的に推奨のマナーです）。\n","\n","----\n","\n","## お願い\n","AIによる画像生成、仕事がなくなるといった文脈で煽る方向の流れは、望むものではありません。\n","むしろ、みんなで「新しい創作」はどういうものか？アーティストはどうAIを使いこなしていけばいいのか？を模索していければお思います。 \n","\n","活版印刷が著作権の概念を生み、写真が印象派や抽象芸術の扉を開いたように、新しいテクノロジーは、新しい表現をもたらします。今、必要なことは、みんなであらゆる方向から実験をして、新しい可能性の総当たり探索をすることだと思います。\n","\n","そんな方向性で使ってもらえればと。\n"]},{"cell_type":"markdown","metadata":{"id":"-9MvuOa_Bg9H"},"source":["## 謝辞\n","構成コードは以下の方々のライブラリ、スニペット、コードを参考、あるいは依拠しています。\n","またnotebookは下記コード群のライセンスを継承します。\n","\n","* [StableDiffusion](https://github.com/CompVis/stable-diffusion) - [CreativeML Open RAIL-M License](https://github.com/CompVis/stable-diffusion/blob/main/LICENSE)\n","* [Diffusers](https://github.com/huggingface/diffusers) - [Apache License 2.0](https://github.com/huggingface/diffusers/blob/main/LICENSE)\n","* KLMSサンプリングは、[@RiversHaveWings](https://twitter.com/RiversHaveWings) 氏の [KLMS Sampling](https://github.com/crowsonkb/k-diffusion.git)より。 [MIT License](https://github.com/crowsonkb/k-diffusion/blob/master/LICENSE)\n","* プロンプトのウェイト処理は、[@Lincoln Stein](https://github.com/lstein)氏のカスタム版[StableDiffusion](https://github.com/lstein/stable-diffusion)より。 [MIT LICENSE](https://github.com/lstein/stable-diffusion/blob/main/LICENSE)\n","* KLMS連携の理解に [@pharmapsychotic](https://twitter.com/pharmapsychotic)氏の[Stable Diffusion notebook](https://colab.research.google.com/github/pharmapsychotic/ai-notebooks/blob/main/pharmapsychotic_Stable_Diffusion.ipynb#scrollTo=UU52ZvES6-1T)。"]},{"cell_type":"markdown","metadata":{"id":"QYSWe7iUKRc9"},"source":["# セットアップ"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rcGfQNeFPP6h"},"outputs":[],"source":["#@markdown ## 1-1. Google Driveのマウント\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9RQ_rTSiMPpr"},"outputs":[],"source":["#@markdown ## 1-2. Google Driveにアップしたモデルのパスを設定\n","#@markdown 左メニューからGoogle Driveを掘り、アップロードしたckptファイルを選択し、右クリックから「パスをコピー」を行います。\n","#@markdown コピーしたパスを下のフォームにコピペしてください。\n","\n","GDRIVE_MODEL_PATH = \"/content/drive/MyDrive/stableDiffusion/sd-v1-4.ckpt\" #@param {type:\"string\"}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I54Hq4Ce7ddY"},"outputs":[],"source":["#@markdown ## 1-3. 画像の保存設定\n","DRIVE_PATH = \"/content/drive/MyDrive\" #Driveのルート\n","SAVE_FILE = False #@param {type:\"boolean\"}\n","SAVE_FILE_PATH = \"/content/drive/MyDrive/stable-diffusion/output\" #@param {type:\"string\"}\n","SAVE_FILE_PREFIX = \"SD\" #@param {type:\"string\"}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uc5OwvKdjRJF"},"outputs":[],"source":["#GPUの確認\n","!nvidia-smi -L"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w_u08HhXKZdq"},"outputs":[],"source":["#必要ファイルのインストール\n","%cd /content/\n","\n","#GIT\n","!git clone https://github.com/CompVis/stable-diffusion  \n","%cd /content/stable-diffusion\n","\n","!git clone https://github.com/CompVis/taming-transformers\n","!git clone https://github.com/openai/CLIP.git\n","!git clone https://github.com/crowsonkb/k-diffusion.git\n","!git clone https://github.com/iodoform/g-stable-diffusion.git\n","\n","#PIP\n","!pip install albumentations \n","!pip install diffusers==0.2.4 \n","!pip install gradio \n","!pip install numpy einops kornia\n","!pip install omegaconf\n","!pip install pytorch-lightning\n","!pip install torch-fidelity\n","!pip install transformers\n","!pip install ftfy jsonmerge resize-right torchdiffeq tqdm\n","\n","#Pathを通す\n","import sys\n","sys.path.append(\".\")\n","sys.path.append(\"./CLIP\")\n","sys.path.append('./taming-transformers')\n","sys.path.append('./k-diffusion')\n","sys.path.append('./ldm')\n","\n","#k_diffusionは初期化が必要\n","!echo '' > ./k-diffusion/k_diffusion/__init__.py"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bcHsbr3hblrk"},"outputs":[],"source":["import argparse, gc, json, os, random, sys, time, glob, requests\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","import PIL\n","from contextlib import contextmanager, nullcontext\n","from einops import rearrange, repeat\n","from IPython.display import display, clear_output\n","from itertools import islice\n","from omegaconf import OmegaConf\n","from PIL import Image\n","from pytorch_lightning import seed_everything\n","from torch.cuda.amp import autocast\n","from ldm.util import instantiate_from_config\n","from ldm.models.diffusion.ddim import DDIMSampler\n","from ldm.models.diffusion.plms import PLMSSampler\n","\n","from k_diffusion.sampling import sample_lms\n","from k_diffusion.external import CompVisDenoiser\n","\n","from diffusers.pipelines.stable_diffusion.safety_checker import StableDiffusionSafetyChecker\n","from transformers import AutoFeatureExtractor\n","\n","from datetime import datetime\n","import gc\n","\n","#メモリのクリーンアップ\n","def clear_memory():\n","  gc.collect()\n","  torch.cuda.empty_cache() \n","\n","\n","#新しいDenoiser\n","class CFGDenoiser(nn.Module):\n","    def __init__(self, model):\n","        super().__init__()\n","        self.inner_model = model\n","\n","    def forward(self, x, sigma, uncond, cond, cond_scale):\n","        x_in = torch.cat([x] * 2)\n","        sigma_in = torch.cat([sigma] * 2)\n","        cond_in = torch.cat([uncond, cond])\n","        uncond, cond = self.inner_model(x_in, sigma_in, cond=cond_in).chunk(2)\n","        return uncond + (cond - uncond) * cond_scale\n","\n","\n","#Config用クラス\n","class SDOption():\n","  def __init__(self):\n","    self.ckpt = GDRIVE_MODEL_PATH\n","    self.config = 'configs/stable-diffusion/v1-inference.yaml'\n","    self.ddim_eta = 0.0\n","    self.ddim_steps = 50\n","    self.embedding = None # TextInversion対応用のEmbedding.pyファイルへのパス\n","    self.fixed_code = True\n","    self.init_img = None\n","    self.init_mask = None\n","    self.n_iter = 1\n","    self.n_samples = 1\n","    self.outdir = SAVE_FILE_PATH\n","    self.precision = 'full' # 'autocast'\n","    self.prompt = \"\"\n","    self.sampler = 'klms'\n","    self.save = True\n","    self.scale = 7.5\n","    self.seed = -1\n","    self.strength = 0.5\n","    self.variations_mode = True #Export variations of init Image\n","    self.H = 512\n","    self.W = 512\n","    self.C = 4\n","    self.f = 8\n","    self.prompt_conditioning = None #promptをテキストでなくconditioningで直接注入する場合\n","\n","\n","#第3パラメーターにEmbeddingPath（Text InversionでトレーニングしたTextEmbedderを指定可能に）\n","class SDHelper():\n","  def __init__(self, config_path, model_path, embedding_path = None):\n","    config = OmegaConf.load(config_path)\n","    self.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","    self.model = self.load_model_from_config(config, model_path).to(self.device)\n","    self.safety_model_id = \"CompVis/stable-diffusion-safety-checker\"\n","    self.safety_feature_extractor = AutoFeatureExtractor.from_pretrained(self.safety_model_id)\n","    self.safety_checker = StableDiffusionSafetyChecker.from_pretrained(self.safety_model_id)\n","\n","    if embedding_path != None:\n","      if hasattr(self.model, \"embedding_manager\"):\n","        self.model.embedding_manager.load(embedding_path)\n","      else:\n","        print(\"Warning: Model does not have embedding_manager\")\n","\n","\n","  def chunk(self, it, size):\n","      it = iter(it)\n","      return iter(lambda: tuple(islice(it, size)), ())\n","\n","\n","  def load_model_from_config(self, config, ckpt, verbose=False):\n","      print(f\"Loading model from {ckpt}\")\n","      pl_sd = torch.load(ckpt, map_location=\"cpu\")\n","      if \"global_step\" in pl_sd:\n","          print(f\"Global Step: {pl_sd['global_step']}\")\n","      sd = pl_sd[\"state_dict\"]\n","      model = instantiate_from_config(config.model)\n","      m, u = model.load_state_dict(sd, strict=False)\n","      if len(m) > 0 and verbose:\n","          print(\"missing keys:\")\n","          print(m)\n","      if len(u) > 0 and verbose:\n","          print(\"unexpected keys:\")\n","          print(u)\n","\n","      model.cuda()\n","      model.eval()\n","      return model\n","\n","  def make_batch(self, image, mask):\n","      image = np.array(Image.open(image).convert(\"RGB\"))\n","      image = image.astype(np.float32)/255.0\n","      image = image[None].transpose(0,3,1,2)\n","      image = torch.from_numpy(image)\n","\n","      mask = np.array(Image.open(mask).convert(\"L\"))\n","      mask = mask.astype(np.float32)/255.0\n","      mask = mask[None,None]\n","      mask[mask < 0.5] = 0\n","      mask[mask >= 0.5] = 1\n","      mask = torch.from_numpy(mask)\n","\n","      masked_image = (1-mask)*image\n","\n","      batch = {\"image\": image, \"mask\": mask, \"masked_image\": masked_image}\n","      for k in batch:\n","          batch[k] = batch[k].to(device=self.device)\n","          batch[k] = batch[k]*2.0-1.0\n","      return batch\n","\n","\n","  def load_img(self, path, w, h):\n","    if path.startswith('http://') or path.startswith('https://'):\n","        image = Image.open(requests.get(path, stream=True).raw).convert('RGB')\n","    else:\n","        if os.path.isdir(path):\n","            files = [file for file in os.listdir(path) if file.endswith('.png') or file .endswith('.jpg')]\n","            path = os.path.join(path, random.choice(files))\n","            print(f\"Chose random init image {path}\")\n","        image = Image.open(path).convert('RGB')\n","    image = image.resize((w, h), Image.LANCZOS)\n","    w, h = image.size\n","    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n","    image = image.resize((w, h), resample=PIL.Image.LANCZOS)\n","    image = np.array(image).astype(np.float32) / 255.0\n","    image = image[None].transpose(0, 3, 1, 2)\n","    image = torch.from_numpy(image)\n","    return 2.*image - 1.\n","\n","\n","  def numpy_to_pil(self, images):\n","    if images.ndim == 3:\n","        images = images[None, ...]\n","    images = (images * 255).round().astype(\"uint8\")\n","    pil_images = [Image.fromarray(image) for image in images]\n","    return pil_images\n","\n","\n","  def load_replacement(self, x):\n","      try:\n","          hwc = x.shape\n","          #セーフフィルターのリック・ストレイは、日本では法律に触れる可能性があるので、違う画像に差し替えます。\n","          #y = Image.open(\"assets/rick.jpeg\").convert(\"RGB\").resize((hwc[1], hwc[0]))\n","          y = PIL.Image.new(mode=\"RGB\", size=(hwc[1], hwc[0]))\n","          y = (np.array(y)/255.0).astype(x.dtype)\n","          assert y.shape == x.shape\n","          return y\n","      except Exception:\n","          return x\n","\n","\n","  def check_safety(self, x_image):\n","    safety_checker_input = self.safety_feature_extractor(self.numpy_to_pil(x_image), return_tensors=\"pt\")\n","    x_checked_image, has_nsfw_concept = self.safety_checker(images=x_image, clip_input=safety_checker_input.pixel_values)\n","    assert x_checked_image.shape[0] == len(has_nsfw_concept)\n","    for i in range(len(has_nsfw_concept)):\n","        if has_nsfw_concept[i]:\n","            x_checked_image[i] = self.load_replacement(x_checked_image[i])\n","    return x_checked_image, has_nsfw_concept\n","\n","\n","  def get_prompt_weight(self, prompt):\n","    return self.model.get_learned_conditioning(prompt)\n","    \n","\n","  def generate(self, opt):\n","      global sample_idx\n","      seed_everything(opt.seed)\n","\n","      #出力ディレクトリの作成\n","      os.makedirs(opt.outdir, exist_ok=True)\n","\n","      #サンプラー選択\n","      if opt.sampler == 'plms':\n","          sampler = PLMSSampler(self.model)\n","      else:\n","          sampler = DDIMSampler(self.model)\n","\n","      model_wrap = CompVisDenoiser(self.model)       \n","      batch_size = opt.n_samples\n","\n","      #promptをバッチの数だけコピー\n","      prompt = opt.prompt\n","      assert prompt is not None\n","      data = [batch_size * [prompt]]\n","\n","\n","      #初期画像の潜在空間を作成\n","      init_latent = None\n","      if opt.init_img != None and opt.init_img != '':\n","          init_image = self.load_img(opt.init_img, opt.W, opt.H).to(self.device)\n","          init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)\n","          init_latent = self.model.get_first_stage_encoding(self.model.encode_first_stage(init_image))  # move to latent space\n","\n","\n","      #Inpaint用（つくりかけ）\n","      \"\"\"\n","      if opt.init_mask != None:\n","        init_image = self.load_img(opt.init_img, opt.W, opt.H).to(self.device)\n","        init_mask = self.load_img(opt.init_mask, opt.W, opt.H).to(self.device)\n","        init_masked_image = (1-init_mask)*init_image\n","        batch = {\"image\": init_image, \"mask\": init_mask, \"masked_image\": init_masked_image}\"\"\"\n","\n","      sampler.make_schedule(ddim_num_steps=opt.ddim_steps, ddim_eta=opt.ddim_eta, verbose=False)\n","      t_enc = int(opt.strength * opt.ddim_steps)\n","\n","\n","      #?? txt2imgで使われる初期値っぽいが…？\n","      start_code = None\n","      if opt.fixed_code and init_latent == None:\n","          start_code = torch.randn([opt.n_samples, opt.C, opt.H // opt.f, opt.W // opt.f], device=self.device)\n","\n","      precision_scope = autocast if opt.precision == \"autocast\" else nullcontext\n","\n","      images = []\n","      with torch.no_grad():\n","          with precision_scope(\"cuda\"):\n","              with self.model.ema_scope():\n","                  for n in range(opt.n_iter):\n","                      for prompts in data:\n","                          #init_latentに初期画像の潜在空間\n","                          #cにコンディショナル条件の潜在空間\n","                          #ucにあんコンディショナルの潜在\n","\n","                          uc = None\n","                          if opt.scale != 1.0:\n","                              uc = self.model.get_learned_conditioning(batch_size * [\"\"])\n","\n","                          if isinstance(prompts, tuple):\n","                              prompts = list(prompts)\n","\n","                          #プロンプトのウェイト処理\n","                          #全てのプロンプトに正規化したウェイトをかけて合算する\n","                          subprompts, weights = SDHelper.prompt_splitter(prompts[0])\n","                          if len(subprompts) > 1:\n","                            c = torch.zeros_like(uc)\n","                            # get total weight for normalizing\n","                            totalWeight = sum(weights)\n","                            # normalize each \"sub prompt\" and add it\n","                            for i in range(0,len(subprompts)):\n","                              weight = weights[i]\n","                              #if not skip_normalize:\n","                              # skip_normalizeがついてる意図が不明なので外す。\n","                              weight = weight / totalWeight\n","                              c = torch.add(c,self.model.get_learned_conditioning(subprompts[i]), alpha=weight)\n","                          else: # just standard prompt\n","                            c = self.model.get_learned_conditioning(prompts)\n","\n","                          \n","                          #promptをテキストでなく Tensorで直接注入する場合\n","                          if (opt.prompt_conditioning != None):\n","                            c = opt.prompt_conditioning\n","\n","\n","                          if init_latent != None:\n","                              #Img2Ima\n","                              \n","                              #z_enc に 初期画像の潜在空間\n","                              #cにテキストの潜在空間\n","                              #t_encに 画像のstrength * ddimsteps?\n","                              #ucに空白\n","                              z_enc = sampler.stochastic_encode(init_latent, torch.tensor([t_enc]*batch_size).to(self.device))\n","                              samples = sampler.decode(z_enc, \n","                                                       c, \n","                                                       t_enc, \n","                                                       unconditional_guidance_scale=opt.scale,\n","                                                       unconditional_conditioning=uc)\n","                          else:\n","                              if opt.sampler == 'klms':\n","                                  print(\"Using KLMS sampling\")\n","                                  shape = [opt.C, opt.H // opt.f, opt.W // opt.f]\n","                                  sigmas = model_wrap.get_sigmas(opt.ddim_steps)\n","                                  model_wrap_cfg = CFGDenoiser(model_wrap)\n","                                  x = torch.randn([opt.n_samples, *shape], device=self.device) * sigmas[0]\n","                                  extra_args = {'cond': c, 'uncond': uc, 'cond_scale': opt.scale}\n","                                  samples = sample_lms(model_wrap_cfg, \n","                                                       x, \n","                                                       sigmas, \n","                                                       extra_args=extra_args, \n","                                                       disable=False)\n","                              else:\n","                                  shape = [opt.C, opt.H // opt.f, opt.W // opt.f]\n","                                  samples, _ = sampler.sample(S=opt.ddim_steps,\n","                                                                  conditioning=c,\n","                                                                  batch_size=opt.n_samples,\n","                                                                  shape=shape,\n","                                                                  verbose=False,\n","                                                                  unconditional_guidance_scale=opt.scale,\n","                                                                  unconditional_conditioning=uc,\n","                                                                  eta=opt.ddim_eta,\n","                                                                  x_T=start_code)\n","\n","                          x_samples = self.model.decode_first_stage(samples)\n","                          x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n","                          x_samples = x_samples.cpu().permute(0, 2, 3, 1).numpy()\n","\n","                          #Safety Checker added\n","                          x_checked_image, has_nsfw_concept = self.check_safety(x_samples)\n","                          x_checked_image_torch = torch.from_numpy(x_checked_image).permute(0, 3, 1, 2)\n","\n","                          for x_sample in x_checked_image_torch:\n","                            x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n","                            images.append(Image.fromarray(x_sample.astype(np.uint8)))\n","                            if (opt.save==True):\n","                              file_id = datetime.today().strftime('%Y-%m-%d-%H-%M-%S')\n","                              filepath = os.path.join(opt.outdir, f\"{SAVE_FILE_PREFIX}-{file_id}.png\")\n","                              Image.fromarray(x_sample.astype(np.uint8)).save(filepath)\n","                              #sample_idx += 1\n","      return images\n","\n","  #Prompt Splitter is based on Lincoln Stein's code\n","  #https://github.com/lstein/stable-diffusion/blob/main/ldm/simplet2i.py\n","\n","  #MidJourney互換でプロンプトの重さを処理するコード\n","  #任意の文字列、 :: （スペース入るかも）（数字はいるかも）　（スペース入るかも）\n","  def prompt_splitter(text):\n","    \"\"\"\n","    grabs all text up to the first occurrence of ':' \n","    uses the grabbed text as a sub-prompt, and takes the value following ':' as weight\n","    if ':' has no value defined, defaults to 1.0\n","    repeats until no text remaining\n","    \"\"\"\n","    remaining = len(text)\n","    prompts = []\n","    weights = []\n","    while remaining > 0:\n","        if \"::\" in text:\n","            idx = text.index(\"::\") # first occurrence from start\n","            # grab up to index as sub-prompt\n","            prompt = text[:idx]\n","            remaining -= idx\n","            # remove from main text\n","            text = text[idx+2:]\n","            # find value for weight \n","            if \" \" in text:\n","                idx = text.index(\" \") # first occurence\n","            else: # no space, read to end\n","                idx = len(text)\n","            if idx != 0:\n","                try:\n","                    weight = float(text[:idx])\n","                except: # couldn't treat as float\n","                    print(f\"Warning: '{text[:idx]}' is not a value, are you missing a space?\")\n","                    weight = 1.0\n","            else: # no value found\n","                weight = 1.0\n","            # remove from main text\n","            remaining -= idx\n","            text = text[idx+1:]\n","            # append the sub-prompt and its weight\n","            prompts.append(prompt)\n","            weights.append(weight)\n","        else: # no : found\n","            if len(text) > 0: # there is still text though\n","                # take remainder as weight 1\n","                prompts.append(text)\n","                weights.append(1.0)\n","            remaining = 0\n","    print(prompts)\n","    print(weights)\n","    return prompts, weights\n","\n","\n","#SDHelperのインスタンス化\n","#第3引数にEmbeddingの.ptを指定すれば、TextInversionに対応可能\n","opt = SDOption()\n","sdh = SDHelper(opt.config, opt.ckpt, \"/content/drive/MyDrive/stable-diffusion/embeddings_gs-40000.pt\")"]},{"cell_type":"markdown","metadata":{"id":"UDqgT459ioLA"},"source":["# ガチャを起動"]},{"cell_type":"markdown","source":["#バッチ処理用"],"metadata":{"id":"OVu85-Gi2671"}},{"cell_type":"markdown","source":["* Init Image: ベース画像（オプション）\n","* Prompt: 生成用のテキスト\n","* Width: 幅\n","* Height: 高さ\n","* Cfg Scale: テキスト誘導の強さ（初期値 7.5)\n","* Steps: 画像の描き込み時間。多いほど時間がかかり詳細になる。(初期値 50）\n","* Init Image Strength: ベース画像をどれほど残すか（0: 無視 〜 1: オリジナル画像そのまま）\n","* Num: 1回に生成する枚数\n","* Seed: 画像の生成元となる乱数（-1にすると毎回ランダム）"],"metadata":{"id":"LoIRg5p1KYR6"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"w-cJQ8a1Q7ZL"},"outputs":[],"source":["#ガチャを起動\n","import random\n","import torch\n","import gradio as gr\n","\n","import re\n","from PIL import Image, ImageFont, ImageDraw, ImageFilter, ImageOps\n","from io import BytesIO\n","import base64\n","import imageio\n","import numpy as np\n","\n","clear_memory()\n","\n","\n","def diffuse(init_image, prompt, width, height, guidance_scale, steps, init_strength, num, seed):\n","  result = list()\n","  \"\"\"\n","  if init_image != None:\n","    imageio.imwrite(\"data.png\", init_image[\"image\"])\n","    imageio.imwrite(\"data_mask.png\", init_image[\"mask\"]) \n","    init_image = Image.open(\"data.png\")\n","    mask_image = Image.open(\"data_mask.png\")\n","    #display(init_image)\n","    #display(mask_image)\n","  \"\"\"\n","    \n","  opt.init_img = init_image \n","  #init_image[\"image\"]\n","  #opt.init_mask = init_image[\"mask\"]\n","  opt.strength = 1- init_strength\n","  opt.prompt = prompt\n","  opt.W = width\n","  opt.H = height\n","  opt.scale = guidance_scale\n","  opt.ddim_steps = steps\n","  opt.n_iter = 1\n","  opt.save == SAVE_FILE\n","  \n","  for index in range(int(num)):\n","    opt.seed = random.randint(0, 2**32) if seed == -1 else seed\n","    image = sdh.generate(opt)[0]\n","    #display(image)\n","    result.append(image)\n","  return result\n","\n","\n","def image_clear(image_init, strength_sli):\n","  print(\"image clear\", image_init)\n","\n","\n","def image_change(image_init, strength_sli):\n","  if image_init == None:\n","    return gr.Slider.update(visible=False)\n","  return gr.Slider.update(visible=True)\n","\n","\n","def set_image_to_init(images):\n","  if len(images)==0:\n","    return\n","  try:\n","    image_data = re.sub('^data:image/.+;base64,', '', images[0])\n","    image = Image.open(BytesIO(base64.b64decode(image_data)))\n","    return image\n","  except IndexError:\n","    print(\"failed to get image\")\n","    return\n","def drawflame(img,thickness,color):\n","  im = Image.new('RGB', (thickness*2+img.size[0],thickness*2+img.size[1]), color)\n","  im.paste(img,(thickness,thickness))\n","  return im\n","\n","font = ImageFont.truetype('/usr/share/fonts/truetype/liberation/LiberationMono-Bold.ttf', 100)\n","\n","n_img = Image.open(\"/content/stable-diffusion/g-stable-diffusion/card_flame/N.png\")\n","ImageDraw.Draw(n_img).text((10, 0), \"N\", font=font,stroke_width=10,stroke_fill='black')\n","r_img = Image.open(\"/content/stable-diffusion/g-stable-diffusion/card_flame/R.png\")\n","ImageDraw.Draw(r_img).text((10, 0), \"R\", font=font,stroke_width=10,stroke_fill='black')\n","sr_img = Image.open(\"/content/stable-diffusion/g-stable-diffusion/card_flame/SR.png\")\n","ImageDraw.Draw(sr_img).text((10, 0), \"SR\", font=font,stroke_width=10,stroke_fill='black')\n","ssr_img = Image.open(\"/content/stable-diffusion/g-stable-diffusion/card_flame/SSR.png\")\n","ImageDraw.Draw(ssr_img).text((10, 0), \"SSR\", font=font,stroke_width=10,stroke_fill='black')\n","prompt = \"japanese anime of a beaultiful girl,fantasy costume,fantasy background,beautiful composition,cinematic lighting,pixiv,light novel,digital painting,extremely detailed,sharp focus,ray tracing,8k,cinematic postprocessing\""]},{"cell_type":"markdown","source":["#ガチャを回す"],"metadata":{"id":"Rnel6BhRHoUq"}},{"cell_type":"code","source":["#@markdown # プロンプト設定\n","#@markdown プロンプトを変更する場合は、下記のpromptを書き換えて、このセルを実行してください。\n","prompt = \"japanese anime of a beaultiful girl,fantasy costume,fantasy background,beautiful composition,cinematic lighting,pixiv,light novel,digital painting,extremely detailed,sharp focus,ray tracing,8k,cinematic postprocessing\" #@param {type:\"string\"}"],"metadata":{"cellView":"form","id":"yVVhfY9sIHt8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@markdown # 回す\n","#@markdown レアリティはN,R,SR,SSRの4種類です。にぎやかしに数字が入っていますが特に意味はありません。\n","gacha_illustration = diffuse(init_image =None,prompt =prompt, width=512, height=512, guidance_scale=10, steps = 50, init_strength=0, num=1, seed =-1)[0]\n","rearity_num = random.randint(0, 9)\n","rearity = \"\"\n","imgsize = (630,890)\n","\n","if rearity_num==0 or rearity_num==1 or rearity_num==2 or rearity_num==3:\n","  img = n_img\n","elif rearity_num==4 or rearity_num==5 or rearity_num==6:\n","  img = r_img\n","elif rearity_num==7 or rearity_num==8:\n","  img = sr_img\n","else:\n","  img = ssr_img\n","img.paste(drawflame(gacha_illustration, 10,(0,0,0)),(49,110))\n","tmpim = Image.new('RGB', (512,100), (256,256,256))\n","img.paste(drawflame(gacha_illustration, 10,(0,0,0)),(49,110))\n","img.paste(drawflame(tmpim, 5,(0,0,0)),(54,700))\n","font = ImageFont.truetype('/usr/share/fonts/truetype/liberation/LiberationMono-Bold.ttf', 50)\n","ImageDraw.Draw(img).text((290, 725),str(random.randint(0,9)), font=font,stroke_width=10,stroke_fill='black')\n","drawflame(img,10,(0,0,0))"],"metadata":{"cellView":"form","id":"KezLKT8spA4T"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"machine_shape":"hm","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}